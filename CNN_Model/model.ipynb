{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module Import\n",
        "## Module\n",
        "\n",
        "\n",
        "* numpy : ndarray\n",
        "* cv2 : image resize, 불러오기 등의 처리\n",
        "* glob : 디렉토리 파일 탐색\n",
        "* tensorflow : 모듈 설계\n",
        "* itertools : list iterate"
      ],
      "metadata": {
        "id": "_S3m2Ayl54DA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oxye4JV62b0C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from itertools import chain, repeat, cycle\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab 드라이브 마운트\n",
        "##### 이후 $ cd gdrive/MyDrive/경로.. 로 기본 디렉토리 수정하여 사용\n",
        "##### 이미지 데이터를 가지고 와서 train data 라벨링 진행해야 함"
      ],
      "metadata": {
        "id": "cIeEwiRv6PfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct7XkwXg3Q35",
        "outputId": "93c6ffbc-ec3e-46de-85cd-98ee8b524f16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd gdrive/MyDrive/KT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUerRhDF4Vyv",
        "outputId": "76d77e12-4e12-4691-839f-d64388c53bc1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/KT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이미지 불러오기\n",
        "##### glob.glob(path) 함수 결과를 sort하여 img데이터 들어오는 순서 고정\n",
        "##### cv2.imread() 함수로 img를 읽어오게 됨\n",
        "  - cv2.IMREAD_COLOR : 컬러 이미지 불러올 때 쓰는 옵션\n",
        "##### cv2.resize() : 크기 조절\n",
        "  - dsize를 고정함으로써 원하는 크기로 불러온 이미지 데이터 수정 가능\n",
        "\n",
        "##### 256.0으로 나누어 float데이터 타입을 유지하면서 normalize\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aaobow4Z6aq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get img list from path\n",
        "image_folders = sorted(glob.glob(\"Image/*\"))"
      ],
      "metadata": {
        "id": "dk5R2eBZ2q25"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_folders.remove('Image/info')"
      ],
      "metadata": {
        "id": "-FqB1E2hxb5Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = []\n",
        "each_class_num = []\n",
        "for folder in image_folders:\n",
        "  temp = sorted(glob.glob(folder + \"/*.jpg\"))\n",
        "  temp.extend(sorted(glob.glob(folder + \"/*.png\")))\n",
        "  temp.extend(sorted(glob.glob(folder + \"/*.jpeg\")))\n",
        "  files.extend(temp)\n",
        "  each_class_num.append(len(temp))"
      ],
      "metadata": {
        "id": "ceGn9zm94pFi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "each_class_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g7qW0KW2tJE",
        "outputId": "de23da40-6455-4ad0-f3d0-9092ae209316"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[60, 50, 50, 50]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read image from files list & reshape\n",
        "# train_img shape : ( , 256, 256, 3)\n",
        "train_img = np.array(np.array([cv2.resize(cv2.imread(file, cv2.IMREAD_COLOR), dsize=(256, 256),\n",
        "                                 interpolation=cv2.INTER_LINEAR).astype(np.float64) for file in files]))"
      ],
      "metadata": {
        "id": "A9AX9HYa2tFU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_num = train_img.shape[0]\n",
        "labels = 4\n",
        "train_label = np.array(list(chain.from_iterable((repeat(n, k) for (n, k) in zip(range(labels), each_class_num)))))"
      ],
      "metadata": {
        "id": "YoqgOQ91WL7z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_img.shape)\n",
        "print(train_img[0].shape)\n",
        "print(train_label.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpEZlnkQ2uiU",
        "outputId": "3a50002c-552e-4c83-b75f-7a100c2ce39c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(210, 256, 256, 3)\n",
            "(256, 256, 3)\n",
            "(210,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize\n",
        "train_img = train_img / (256.0)"
      ],
      "metadata": {
        "id": "VplUw2wA2v2b"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(train_img, train_label, test_size=0.1, stratify=train_label, random_state=34)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, stratify=y_train, random_state=12)"
      ],
      "metadata": {
        "id": "ZgOhCJ3kzqW7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTRfdCxD35zX",
        "outputId": "0fbf7d4a-2c9d-4342-a1fe-65651c9e03a9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(170, 256, 256, 3)\n",
            "(170,)\n",
            "(21, 256, 256, 3)\n",
            "(21,)\n",
            "(19, 256, 256, 3)\n",
            "(19,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture\n",
        "\n",
        "**코드가 조금 더러움 주의**\n",
        "\n",
        "기본적으로 ResNet의 구조를 가지고 있는 NN으로 구성\n",
        "## Hyperparameters\n",
        "##### **수정 불가능한 hyperparameter**\n",
        "  - input_img_shape : 256*256으로 고정\n",
        "  - input_channel : RGB 3채널 이미지\n",
        "\n",
        "##### **수정 가능한 hyperparmeter**\n",
        "  - layer1_output_channel : layer1 의 output channel개수\n",
        "  - layer1_kernel_size : ResNet 처음 conv kernel_size는 7*7 이용\n",
        "  - layer2_output_channel : layer2 의 output channel개수 (== layer1_output_channel)\n",
        "  - layer3_output_channel : layer3 의 output channel개수 (layer2_output_channel * 2)\n",
        "  - res_kernel_size : resNet에서는 기본적으로 첫번째 conv 제외 3*3 kernel로 고정\n",
        "\n",
        "## Layers\n",
        "1. layer1 : convolution + pooling\n",
        "2. layer2 : conv + norm + relu + conv + norm + relu + add\n",
        "3. layer3 : conv + norm + relu + conv + norm + relu + identity + add\n",
        "4. flatten : 분류기에 넣기 위해 flatten진행\n",
        "5. classifier : 분류기, FC layer이용, ResNet에서는 FC layer의 Weight개수가 많으므로 한개의 FC layer만 사용\n",
        "\n",
        "### 1*1 Convolution\n",
        "layer2에서 사용하는 res_unit2의 경우, 이미지 데이터가 들어갈 때 layer2_output_channel개로 들어가지만, 나올 때 layer3_output_channel로 나오게 되는데, channel개수를 맞추어주기 위해서 convolution진행.\n",
        "이미지 크기도 res_unit2의 첫번째 convolution에서 stride (2, 2)로 진행하므로 반으로 줄어들게 되므로 1*1 convolution도 stride (2, 2)로 진행"
      ],
      "metadata": {
        "id": "9fonqlP-7Oat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "global input_img_shape, input_channel, layer1_output_channel, layer1_kernel_size, res_kernel_size, layer2_output_channel, layer3_output_channel\n",
        "input_img_shape = (256, 256, 3)\n",
        "input_channel = 3\n",
        "layer1_output_channel = 6\n",
        "layer1_kernel_size = (15, 15)\n",
        "layer2_output_channel = 6\n",
        "layer3_output_channel = 12\n",
        "\n",
        "# overall residual units num = num_residual_units * num_residual_layers \n",
        "res_kernel_size = (21, 21)"
      ],
      "metadata": {
        "id": "83CKVEPeR1TD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.activations import softmax\n",
        "class cnn_model(tf.keras.Model):\n",
        "  global input_img_shape, input_channel, layer1_output_channel, layer1_kernel_size, res_kernel_size, layer2_output_channel, layer3_output_channel\n",
        "  def __init__(self):\n",
        "    super(cnn_model, self).__init__()\n",
        "    self.num_residual_units = 3 # number of units per layer\n",
        "    self.num_residual_layers = 2 # number of layer per model\n",
        "    self.res_kernel_size = (3, 3)\n",
        "    \n",
        "    self.conv1 = layers.Conv2D(layer1_output_channel, layer1_kernel_size, strides=(2, 2), \n",
        "                       padding='same', input_shape=input_img_shape, activation='relu')\n",
        "    self.pool1 = layers.MaxPool2D(pool_size=(4, 4))\n",
        "    self.conv2_t = layers.Conv2D(layer2_output_channel, res_kernel_size, padding='same')\n",
        "    self.batchnorm1 = layers.BatchNormalization()\n",
        "    self.relu1 = layers.ReLU()\n",
        "    self.conv2 = layers.Conv2D(layer2_output_channel, res_kernel_size, padding='same')\n",
        "    self.batchnorm2 = layers.BatchNormalization()\n",
        "    self.relu2 = layers.ReLU()\n",
        "    self.add1 = layers.Add()\n",
        "    \n",
        "    self.conv3 = layers.Conv2D(layer3_output_channel, res_kernel_size, strides=(2, 2), padding='same')\n",
        "    self.batchnorm3 = layers.BatchNormalization()\n",
        "    self.relu3 = layers.ReLU()\n",
        "    \n",
        "    self.conv4 = layers.Conv2D(layer3_output_channel, res_kernel_size, padding='same')\n",
        "    self.batchnorm4 = layers.BatchNormalization()\n",
        "    self.relu4 = layers.ReLU()\n",
        "    self.add2 = layers.Add()\n",
        "    \n",
        "    self.identity = layers.Conv2D(layer3_output_channel, (1, 1), padding='same', strides=(2, 2))\n",
        "\n",
        "    self.flat = layers.Flatten()\n",
        "    self.fc1 = layers.Dense(4)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.pool1(x)\n",
        "    x_t = self.conv2_t(x)\n",
        "    x_t = self.batchnorm1(x_t)\n",
        "    x_t = self.relu1(x_t)\n",
        "    x_t = self.conv2(x_t)\n",
        "    x_t = self.batchnorm2(x_t)\n",
        "    x_t = self.relu2(x_t)\n",
        "    x = self.add1([x, x_t])\n",
        "    #print(x.shape)\n",
        "    x_t = self.conv3(x_t)\n",
        "    x_t = self.batchnorm3(x_t)\n",
        "    x_t = self.relu3(x_t)\n",
        "    x_t = self.conv4(x_t)\n",
        "    x_t = self.batchnorm4(x_t)\n",
        "    x_t = self.relu4(x_t)\n",
        "    x = self.identity(x)\n",
        "    x = self.add2([x, x_t])\n",
        "    x = self.flat(x)\n",
        "    x = self.fc1(x)\n",
        "    probs = softmax(x)\n",
        "    return probs"
      ],
      "metadata": {
        "id": "892Gl70xF178"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn_model()"
      ],
      "metadata": {
        "id": "m_F-B1R6A2g8"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model optimizer, loss등 정의 compile"
      ],
      "metadata": {
        "id": "Ct65qbqsyrX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "L8gudp5MT9Y7"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "sT8fbjD4yxjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Hyperparameters\n",
        "epoch = 50\n",
        "batch = 10"
      ],
      "metadata": {
        "id": "v6KAI2UG4gl8"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEFStN575NmW",
        "outputId": "333b3081-d32d-4db7-f5f7-d82612b458ea"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(170, 256, 256, 3)\n",
            "(170,)\n",
            "(21, 256, 256, 3)\n",
            "(21,)\n",
            "(19, 256, 256, 3)\n",
            "(19,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'checkpoint.ckpt'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "earlystopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "# , callbacks=[checkpoint, earlystopping]"
      ],
      "metadata": {
        "id": "kYyYZywR5vYQ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=epoch, validation_data=(x_val, y_val) , callbacks=[checkpoint, earlystopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZXKBeRnUNTu",
        "outputId": "b85cf451-cfe9-4735-968d-5e40747b8b2f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 1.5279 - accuracy: 0.4412\n",
            "Epoch 00001: val_loss improved from inf to 1.27050, saving model to checkpoint.ckpt\n",
            "6/6 [==============================] - 10s 1s/step - loss: 1.5279 - accuracy: 0.4412 - val_loss: 1.2705 - val_accuracy: 0.4737\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.6647 - accuracy: 0.7412\n",
            "Epoch 00002: val_loss improved from 1.27050 to 1.26044, saving model to checkpoint.ckpt\n",
            "6/6 [==============================] - 11s 2s/step - loss: 0.6647 - accuracy: 0.7412 - val_loss: 1.2604 - val_accuracy: 0.2632\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4616 - accuracy: 0.8471\n",
            "Epoch 00003: val_loss did not improve from 1.26044\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.4616 - accuracy: 0.8471 - val_loss: 1.3304 - val_accuracy: 0.3158\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.8529\n",
            "Epoch 00004: val_loss improved from 1.26044 to 1.12414, saving model to checkpoint.ckpt\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.3689 - accuracy: 0.8529 - val_loss: 1.1241 - val_accuracy: 0.4737\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.9176\n",
            "Epoch 00005: val_loss improved from 1.12414 to 1.05157, saving model to checkpoint.ckpt\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.2609 - accuracy: 0.9176 - val_loss: 1.0516 - val_accuracy: 0.6842\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9471\n",
            "Epoch 00006: val_loss did not improve from 1.05157\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.1729 - accuracy: 0.9471 - val_loss: 1.3287 - val_accuracy: 0.2632\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 0.9647\n",
            "Epoch 00007: val_loss improved from 1.05157 to 0.89274, saving model to checkpoint.ckpt\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.1229 - accuracy: 0.9647 - val_loss: 0.8927 - val_accuracy: 0.6842\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2127 - accuracy: 0.9235\n",
            "Epoch 00008: val_loss did not improve from 0.89274\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.2127 - accuracy: 0.9235 - val_loss: 0.9992 - val_accuracy: 0.5789\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1485 - accuracy: 0.9353\n",
            "Epoch 00009: val_loss did not improve from 0.89274\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.1485 - accuracy: 0.9353 - val_loss: 1.0338 - val_accuracy: 0.5789\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1740 - accuracy: 0.9471\n",
            "Epoch 00010: val_loss did not improve from 0.89274\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.1740 - accuracy: 0.9471 - val_loss: 1.0088 - val_accuracy: 0.5789\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.9471\n",
            "Epoch 00011: val_loss did not improve from 0.89274\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.1650 - accuracy: 0.9471 - val_loss: 1.3488 - val_accuracy: 0.3684\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0867 - accuracy: 0.9706\n",
            "Epoch 00012: val_loss did not improve from 0.89274\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0867 - accuracy: 0.9706 - val_loss: 1.2755 - val_accuracy: 0.3684\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9882\n",
            "Epoch 00013: val_loss did not improve from 0.89274\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0603 - accuracy: 0.9882 - val_loss: 1.1269 - val_accuracy: 0.4211\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 1.0000\n",
            "Epoch 00014: val_loss improved from 0.89274 to 0.73475, saving model to checkpoint.ckpt\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.7347 - val_accuracy: 0.6842\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9941\n",
            "Epoch 00015: val_loss did not improve from 0.73475\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0322 - accuracy: 0.9941 - val_loss: 0.8564 - val_accuracy: 0.5789\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 1.0000\n",
            "Epoch 00016: val_loss did not improve from 0.73475\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.7811 - val_accuracy: 0.6316\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9941\n",
            "Epoch 00017: val_loss did not improve from 0.73475\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0325 - accuracy: 0.9941 - val_loss: 1.0478 - val_accuracy: 0.4737\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 1.0000\n",
            "Epoch 00018: val_loss did not improve from 0.73475\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.9986 - val_accuracy: 0.5263\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 00019: val_loss improved from 0.73475 to 0.71546, saving model to checkpoint.ckpt\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.7155 - val_accuracy: 0.6842\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 00020: val_loss improved from 0.71546 to 0.62085, saving model to checkpoint.ckpt\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.6209 - val_accuracy: 0.8421\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 00021: val_loss improved from 0.62085 to 0.61376, saving model to checkpoint.ckpt\n",
            "6/6 [==============================] - 11s 2s/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.6138 - val_accuracy: 0.8947\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 00022: val_loss did not improve from 0.61376\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.7070 - val_accuracy: 0.7895\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 00023: val_loss did not improve from 0.61376\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.6892 - val_accuracy: 0.7368\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 00024: val_loss did not improve from 0.61376\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.6630 - val_accuracy: 0.7895\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 00025: val_loss did not improve from 0.61376\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6520 - val_accuracy: 0.7895\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 00026: val_loss did not improve from 0.61376\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6471 - val_accuracy: 0.7895\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 00027: val_loss did not improve from 0.61376\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6469 - val_accuracy: 0.7895\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 00028: val_loss did not improve from 0.61376\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6457 - val_accuracy: 0.7895\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 00029: val_loss did not improve from 0.61376\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6449 - val_accuracy: 0.7895\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 00030: val_loss did not improve from 0.61376\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6489 - val_accuracy: 0.7368\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 00031: val_loss did not improve from 0.61376\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6404 - val_accuracy: 0.7368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_fi = cnn_model()\n",
        "model_fi.load_weights(filename)\n",
        "model_fi.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "BKGxiqTMe2Se"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_fi.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wct1WJOD4uOW",
        "outputId": "19a3b312-772a-486f-aa4e-223edb1f63a3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 750ms/step - loss: 0.4683 - accuracy: 0.9048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('test loss, test acc:', results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuHBsYsk40Qt",
        "outputId": "9a66311e-c4ae-4dd8-c9cf-4395f8d7b720"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss, test acc: [0.4682854115962982, 0.9047619104385376]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGT-cZKhBqvQ",
        "outputId": "a8d54089-9a54-4149-8935-475fd0b6acf5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cnn_model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_24 (Conv2D)          multiple                  4056      \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          multiple                  4362      \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  multiple                 24        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_16 (ReLU)             multiple                  0         \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          multiple                  4362      \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  multiple                 24        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_17 (ReLU)             multiple                  0         \n",
            "                                                                 \n",
            " add_8 (Add)                 multiple                  0         \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          multiple                  8724      \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  multiple                 48        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_18 (ReLU)             multiple                  0         \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          multiple                  17436     \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  multiple                 48        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_19 (ReLU)             multiple                  0         \n",
            "                                                                 \n",
            " add_9 (Add)                 multiple                  0         \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          multiple                  84        \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         multiple                  0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             multiple                  12292     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,460\n",
            "Trainable params: 51,388\n",
            "Non-trainable params: 72\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"CNNMODEL.h5\")"
      ],
      "metadata": {
        "id": "6rbu_72pkdj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dump"
      ],
      "metadata": {
        "id": "lwMfA7Rdyzm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model(train_img)"
      ],
      "metadata": {
        "id": "xU5fCSjdBkgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def residual_unit(self, x, in_channel, out_channel, kernel_size):\n",
        "  #   if in_channel == out_channel:\n",
        "  #     res1 = layers.Conv2D(out_channel, kernel_size, padding='same', strides=(1, 1))\n",
        "  #   else:\n",
        "  #     res1 = layers.Conv2D(out_channel, kernel_size, padding='same', strides=(2, 2))\n",
        "  #   batch1 = layers.BatchNormalization()\n",
        "  #   relu1 = layers.ReLU()\n",
        "  #   res2 = layers.Conv2D(out_channel, kernel_size, padding='same')\n",
        "  #   batch2 = layers.BatchNormalization()\n",
        "  #   relu2 = layers.ReLU()\n",
        "\n",
        "  #   x_t = res1(x)\n",
        "  #   x_t = batch1(x_t)\n",
        "  #   x_t = relu1(x_t)\n",
        "  #   x_t = res2(x_t)\n",
        "  #   x_t = batch2(x_t)\n",
        "  #   x_t = relu2(x_t)\n",
        "\n",
        "  #   if in_channel != out_channel:\n",
        "  #     identity = layers.Conv2D(out_channel, (1, 1), padding='same', strides=(2, 2))\n",
        "  #     x = identity(x)\n",
        "  #   print(x_t.shape)\n",
        "  #   print(x.shape)\n",
        "  #   return x_t + x\n",
        "\n",
        "  # channels = x.shape[3]\n",
        "    # for l in range(self.num_residual_layers):\n",
        "    #   for u in range(self.num_residual_units):\n",
        "    #     x = self.residual_unit(x, channels, (lambda x,y:channels*2 if x != 0 and y == 0 else channels)(l, u), self.res_kernel_size)\n",
        "    #     channels = channels * 2 if l != 0 and u == 0 else channels"
      ],
      "metadata": {
        "id": "MMhj8WxlrPaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape = (256, 256, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "metadata": {
        "id": "MYpwDLKG2xdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "5xgtV31q2yz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model information\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "4zYxbpLf2z_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "#print(model(train_img))"
      ],
      "metadata": {
        "id": "IZ_O3u7E21Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in_channel : input image channel size, out_channel : output image channel of each unit\n",
        "# kernel_size  ex) (3, 3), (5, 5), ..\n",
        "class ResidualUnit(tf.keras.Model):\n",
        "  def __init__(self, in_channel, out_channel, kernel_size, stride=False):\n",
        "    super(ResidualUnit, self).__init__()\n",
        "    if stride:\n",
        "      self.conv1 = layers.Conv2D(out_channel, kernel_size, padding='same', activation='relu', stride=(2, 2))\n",
        "    else:\n",
        "      self.conv1 = layers.Conv2D(out_channel, kernel_size, padding='same', activation='relu')\n",
        "    self.conv2 = layers.Conv2D(out_channel, kernel_size, padding='same', activation='relu')\n",
        "\n",
        "    if in_channel ==  out_channel:\n",
        "      self.identity = lambda x: x\n",
        "    else:\n",
        "      self.identity = layers.Conv2D(out_channel, (1, 1), padding='same')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    return x + self.identity(x)"
      ],
      "metadata": {
        "id": "q6_lrtijI7uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualLayer(tf.keras.Model):\n",
        "  def __init__(self, in_channel, out_channel, kernel_size, stride=False):\n",
        "    super(ResidualLayer, self).__init__()\n",
        "    # residualunit + unit + unit\n",
        "    self.res1 = ResidualUnit(in_channel, out_channel, kernel_size, stride)\n",
        "    self.res2 = ResidualUnit(out_channel, out_channel, kernel_size)\n",
        "    self.res3 = ResidualUnit(out_channel, out_channel, kernel_size)\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = self.res1(x)\n",
        "    x = self.res2(x)\n",
        "    x = self.res3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "15UEkr_7NJtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.activations import softmax\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(layer1_output_channel, (11, 11), activation='relu', input_shape = (256, 256, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(layer2_output_channel, (7, 7), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Conv2D(layer3_output_channel, (7, 7), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(4, activation='softmax'))"
      ],
      "metadata": {
        "id": "w-0NYFJ_O4lk"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.activations import softmax\n",
        "class cnn_model(tf.keras.Model):\n",
        "  global input_img_shape, input_channel, layer1_output_channel, layer1_kernel_size, res_kernel_size, layer2_output_channel, layer3_output_channel\n",
        "  def __init__(self):\n",
        "    super(cnn_model, self).__init__()\n",
        "    self.num_residual_units = 3 # number of units per layer\n",
        "    self.num_residual_layers = 2 # number of layer per model\n",
        "    self.res_kernel_size = (3, 3)\n",
        "\n",
        "    self.layer1 = models.Sequential(\n",
        "        [\n",
        "         layers.Conv2D(layer1_output_channel, layer1_kernel_size, strides=(2, 2), \n",
        "                       padding='same', input_shape=input_img_shape, activation='relu'),\n",
        "         layers.MaxPool2D(pool_size=(4, 4))]\n",
        "    )\n",
        "\n",
        "    self.res_unit1 = models.Sequential(\n",
        "        [layers.Conv2D(layer2_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU(),\n",
        "         layers.Conv2D(layer2_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU()\n",
        "         ]\n",
        "    )\n",
        "    '''\n",
        "\n",
        "    self.res_unit2 = models.Sequential(\n",
        "        [layers.Conv2D(layer3_output_channel, res_kernel_size, strides=(2, 2), padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU(),\n",
        "         layers.Conv2D(layer3_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU()\n",
        "         ]\n",
        "    )\n",
        "\n",
        "    self.res_unit3 = models.Sequential(\n",
        "        [layers.Conv2D(layer3_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU(),\n",
        "         layers.Conv2D(layer3_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU()\n",
        "         ]\n",
        "    )'''\n",
        "\n",
        "    #self.identity = layers.Conv2D(layer3_output_channel, (1, 1), padding='same', strides=(2, 2))\n",
        "\n",
        "    self.flatten_layer = layers.Flatten()\n",
        "\n",
        "    self.classifier = models.Sequential(\n",
        "        [layers.Dense(4)]\n",
        "    )\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x_t = self.res_unit1(x)\n",
        "    x = x + x_t\n",
        "    #x_t = self.res_unit1(x)\n",
        "    #x = x + x_t\n",
        "    #x_t = self.res_unit1(x)\n",
        "    #x = x + x_t\n",
        "    #x_t = self.res_unit2(x)\n",
        "    #x = self.identity(x) + x_t\n",
        "    #x_t = self.res_unit3(x)\n",
        "    #x = x + x_t\n",
        "    #x_t = self.res_unit3(x)\n",
        "    #x = x + x_t\n",
        "    #x_t = self.res_unit3(x)\n",
        "    #x = x + x_t\n",
        "    x = self.flatten_layer(x)\n",
        "    x = self.classifier(x)\n",
        "    probs = softmax(x)\n",
        "    return probs"
      ],
      "metadata": {
        "id": "zKkhFVOmHfkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cnn_model(tf.keras.Model):\n",
        "  global input_img_shape, input_channel, layer1_output_channel, layer1_kernel_size, res_kernel_size, layer2_output_channel, layer3_output_channel\n",
        "  def __init__(self):\n",
        "    super(cnn_model, self).__init__()\n",
        "\n",
        "    self.layer1 = models.Sequential(\n",
        "        [layers.Conv2D(layer1_output_channel, layer1_kernel_size, strides=(2, 2), \n",
        "                       padding='same', input_shape=input_img_shape, activation='relu'),\n",
        "         layers.MaxPool2D(pool_size=(2, 2))]\n",
        "    )\n",
        "\n",
        "    self.unit1 = models.Sequential(\n",
        "        [layers.Conv2D(layer2_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU(),\n",
        "         layers.Conv2D(layer2_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU()\n",
        "         ]\n",
        "    )\n",
        "    '''\n",
        "    self.unit2 = models.Sequential(\n",
        "        [layers.Conv2D(layer2_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU(),\n",
        "         layers.Conv2D(layer2_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU()\n",
        "         ]\n",
        "    )\n",
        "\n",
        "    self.unit3 = models.Sequential(\n",
        "        [layers.Conv2D(layer2_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU(),\n",
        "         layers.Conv2D(layer2_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU()\n",
        "         ]\n",
        "    )\n",
        "    '''\n",
        "\n",
        "    self.unit4 = models.Sequential(\n",
        "        [layers.Conv2D(layer3_output_channel, res_kernel_size, strides=(2, 2), padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU(),\n",
        "         layers.Conv2D(layer3_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU()\n",
        "         ]\n",
        "    )\n",
        "\n",
        "    '''self.unit5 = models.Sequential(\n",
        "        [layers.Conv2D(layer3_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU(),\n",
        "         layers.Conv2D(layer3_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU()\n",
        "         ]\n",
        "    )\n",
        "\n",
        "    self.unit6 = models.Sequential(\n",
        "        [layers.Conv2D(layer3_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU(),\n",
        "         layers.Conv2D(layer3_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU()\n",
        "         ]\n",
        "    )\n",
        "\n",
        "    self.unit7 = models.Sequential(\n",
        "        [layers.Conv2D(layer3_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU(),\n",
        "         layers.Conv2D(layer3_output_channel, res_kernel_size, padding='same'),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ReLU()\n",
        "         ]\n",
        "    )   '''\n",
        "    self.identity = layers.Conv2D(layer3_output_channel, (1, 1), padding='same', strides=(2, 2))\n",
        "    self.flatten_layer = layers.Flatten()\n",
        "\n",
        "    self.classifier = models.Sequential(\n",
        "        [layers.Dense(4)]\n",
        "    )\n",
        "\n",
        "    \n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.unit1(x)\n",
        "    #x = x + x_t\n",
        "    #x_t = self.unit2(x)\n",
        "    #x = x + x_t\n",
        "    #x_t = self.unit3(x)\n",
        "    #x = x + x_t\n",
        "    x = self.unit4(x)\n",
        "    #x = self.identity(x) + x_t\n",
        "    #x_t = self.unit5(x)\n",
        "    #x = x + x_t\n",
        "    #x_t = self.unit6(x)\n",
        "    #x = x + x_t\n",
        "    #x_t = self.unit7(x)\n",
        "    #x = x + x_t\n",
        "    x = self.flatten_layer(x)\n",
        "    x = self.classifier(x)\n",
        "    probs = tf.nn.softmax(x)\n",
        "    return probs"
      ],
      "metadata": {
        "id": "oy7m1ziH8YMZ"
      },
      "execution_count": 392,
      "outputs": []
    }
  ]
}